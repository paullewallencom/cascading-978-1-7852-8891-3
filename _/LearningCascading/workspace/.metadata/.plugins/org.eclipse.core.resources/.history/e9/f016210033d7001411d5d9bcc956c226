/* Copyright (c) 2015, Analytics Inside LLC. All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 *   - Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *
 *   - Redistributions in binary form must reproduce the above copyright
 *     notice, this list of conditions and the following disclaimer in the
 *     documentation and/or other materials provided with the distribution.
 *
 *   - Neither the name of Analytics Inside LLC or the names of its
 *     products or trademarks,  may be used to endorse or promote products 
 *     derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
 * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */ 

package com.ai.learning.jobs.hadoop;

import java.util.Properties;

import com.ai.learning.SentFunc;
import com.ai.learning.TokenBuffer;
import com.ai.learning.NameBuffer;
import cascading.flow.Flow;
import cascading.pipe.Each;
import cascading.pipe.Every;
import cascading.pipe.GroupBy;
import cascading.pipe.Pipe;
import cascading.property.AppProps;
import cascading.scheme.Scheme;
//import cascading.scheme.local.TextDelimited;
import cascading.tap.SinkMode;
import cascading.tap.Tap;
import cascading.tap.hadoop.Hfs;
import cascading.tuple.Fields;
import cascading.flow.hadoop.HadoopFlowConnector;
import cascading.scheme.hadoop.TextDelimited;

//Use input file of a format "docname" [tab] "text" with a header
// Use Chapter8Data.txt in data folder
// Output file is of a format docname, sentnum, name
// args[0] - iput file, args[1] - final output with names
public class TestNLPNameBufferJob {

	public static void main(String[] args) {

		Fields fieldDeclarationInput = new Fields("document","text");
		Fields fieldDeclarationOutputInterim = new Fields( "documentname","sentnumber","wordnum", "word");
		Fields fieldDeclarationOutput = new Fields( "documentname","sentnumber","word");

		Scheme inputScheme = new TextDelimited( fieldDeclarationInput, true,"\t");
		Scheme outputScheme = new TextDelimited( fieldDeclarationOutput, "\t");
		Tap docTap = new Hfs(inputScheme, args[0]);	
		
		Tap finalOutTap = new Hfs(outputScheme, args[1], SinkMode.REPLACE);

		Pipe inPipe = new Pipe("InPipe"); 
		inPipe= new Each(inPipe, new SentFunc());
		// Optional debug
		//inPipe = new Each(inPipe, new Debug());	
		inPipe = new GroupBy(inPipe, new Fields("document"), new Fields("sentnum"));
		inPipe = new Every(inPipe, new TokenBuffer(), fieldDeclarationOutputInterim);	
		inPipe = new GroupBy(inPipe, new Fields("documentname","sentnumber"), new Fields("documentname"));
		String pathPrefix = "hdfs:/user/" + System.getProperty("user.name") + "/";
		inPipe = new Every(inPipe, new NameBuffer<NameBuffer.Context>(pathPrefix + "models/en-ner-person.bin", false), Fields.RESULTS);	
		Properties properties = new Properties();
		AppProps.setApplicationJarClass( properties, com.ai.learning.jobs.hadoop.TestNLPNameBufferJob.class );
		Flow flow = new HadoopFlowConnector().connect( "process", docTap, finalOutTap, inPipe );
		// execute the flow, block until complete
		flow.complete();
		System.out.println("Done");
	}
}
